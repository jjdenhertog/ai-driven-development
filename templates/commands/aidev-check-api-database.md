---
allowed-tools: all
description: Comprehensive API and database call audit, optimization, and cost reduction for Next.js/React/TypeScript projects
---

# üöÄüöÄüöÄ CRITICAL REQUIREMENT: OPTIMIZE ALL API & DATABASE CALLS! üöÄüöÄüöÄ

**THIS IS NOT A REPORTING TASK - THIS IS AN OPTIMIZATION TASK!**

When you run `/aidev-check-api-database`, you are REQUIRED to:

1. **IDENTIFY** all API endpoints, database queries, and external service calls
2. **OPTIMIZE EVERY SINGLE ONE** - not just report them!
3. **USE MULTIPLE AGENTS** to optimize issues in parallel:
   - Spawn one agent to consolidate duplicate API calls
   - Spawn another to implement caching strategies
   - Spawn more agents for query optimization
   - Spawn agents for request batching
   - Say: "I'll spawn multiple agents to optimize all these API/database issues in parallel"
4. **DO NOT STOP** until:
   - ‚úÖ ALL redundant calls are eliminated
   - ‚úÖ NO N+1 query problems exist
   - ‚úÖ ALL requests are properly cached
   - ‚úÖ ALL queries are optimized
   - ‚úÖ NO overfetching of data
   - ‚úÖ MINIMAL API usage costs
   - ‚úÖ EVERYTHING is EFFICIENT

**FORBIDDEN BEHAVIORS:**

- ‚ùå "Here are the inefficient API calls I found" ‚Üí NO! OPTIMIZE THEM!
- ‚ùå "This component makes multiple requests" ‚Üí NO! CONSOLIDATE THEM!
- ‚ùå "Database queries could be faster" ‚Üí NO! MAKE THEM FASTER!
- ‚ùå "API usage might be expensive" ‚Üí NO! REDUCE THE COST!
- ‚ùå Stopping after listing inefficiencies ‚Üí NO! KEEP WORKING!

**MANDATORY WORKFLOW:**

```
1. Audit all API/database usage ‚Üí Find inefficiencies
2. IMMEDIATELY spawn agents to optimize ALL issues
3. Re-run performance checks ‚Üí Find remaining issues
4. Optimize those too
5. REPEAT until EVERYTHING is efficient
```

**YOU ARE NOT DONE UNTIL:**

- Zero duplicate API calls across components
- All database queries use proper indexing
- All data fetching is cached appropriately
- No waterfall loading patterns
- Minimal API request counts
- Optimized payload sizes
- Cost-effective API usage
- Everything shows optimal performance

---

‚ö° **MANDATORY API/DATABASE PRE-FLIGHT CHECK** ‚ö°

1. Re-read CLAUDE.md RIGHT NOW
2. Re-read PROJECT.md to understand architecture
3. Check current .claude/TODO.md status
4. Verify you're not declaring "optimized" prematurely

Execute comprehensive API/database audit with ZERO tolerance for inefficiency.

**FORBIDDEN EXCUSE PATTERNS:**

- "This is how the API works" ‚Üí NO, find a better way
- "The database is fast enough" ‚Üí NO, make it faster
- "Users won't notice the delay" ‚Üí NO, every millisecond counts
- "It's a third-party limitation" ‚Üí NO, work around it
- "Caching is complex" ‚Üí NO, implement it anyway
- "The cost is reasonable" ‚Üí NO, minimize it further

Let me ultrathink about optimizing this codebase's external communications.

üö® **REMEMBER: Every unnecessary API call costs money and degrades performance!** üö®

**Universal API/Database Verification Protocol:**

**Step 0: Documentation Discovery**

- Search for API documentation URLs in codebase
- Identify all external services being used
- Fetch and analyze API documentation for each service
- Understand rate limits, pricing, and best practices
- Document all database schemas and relationships

**Step 1: API & Database Discovery Scan**

Comprehensive audit of all external calls:

```bash
# Find all API endpoints
grep -r "fetch\|axios\|http" --include="*.ts" --include="*.tsx" --include="*.js" .
grep -r "API_URL\|ENDPOINT" --include="*.ts" --include="*.tsx" .
grep -r "process.env.*API\|process.env.*URL" .

# Find database queries
grep -r "prisma\|query\|findMany\|findUnique" --include="*.ts" .
grep -r "SELECT\|INSERT\|UPDATE\|DELETE" --include="*.ts" .
grep -r "mongoose\|mongodb\|postgres\|mysql" --include="*.ts" .

# Find GraphQL queries
grep -r "gql\|useQuery\|useMutation" --include="*.ts" --include="*.tsx" .
```

**Step 2: Performance Analysis**

Identify performance bottlenecks:

- [ ] Map all API calls and their frequencies
- [ ] Identify duplicate or similar requests
- [ ] Find N+1 query problems
- [ ] Detect waterfall loading patterns
- [ ] Measure request/response sizes
- [ ] Calculate API usage costs
- [ ] Analyze query execution times
- [ ] Check for missing database indexes

**Frontend API Optimization Requirements:**

- NO duplicate API calls for same data
- NO sequential requests that could be batched
- NO fetching unused data fields
- NO missing loading states during requests
- NO unhandled error states
- NO requests without proper caching
- NO large payload transfers
- NO polling when websockets available

**Backend/Database Optimization Requirements:**

- ALL queries use proper indexes
- ALL queries avoid N+1 problems
- ALL queries select only needed fields
- ALL batch operations use transactions
- ALL frequently accessed data is cached
- ALL connection pooling is optimized
- ALL queries are parameterized
- ALL pagination is implemented efficiently

**Step 3: API Call Consolidation**

Optimize API usage patterns:

**‚ùå INEFFICIENT - Multiple calls:**
```typescript
// Bad: Three separate API calls
const user = await fetch('/api/user/123');
const posts = await fetch('/api/user/123/posts');
const comments = await fetch('/api/user/123/comments');
```

**‚úÖ OPTIMIZED - Single batched call:**
```typescript
// Good: One call with includes
const userData = await fetch('/api/user/123?include=posts,comments');
// OR use GraphQL to fetch exactly what's needed
```

**‚ùå INEFFICIENT - Waterfall pattern:**
```typescript
// Bad: Sequential loading
const users = await getUsers();
for (const user of users) {
  user.profile = await getProfile(user.id); // N+1 problem!
}
```

**‚úÖ OPTIMIZED - Batch loading:**
```typescript
// Good: Batch fetch profiles
const users = await getUsers();
const userIds = users.map(u => u.id);
const profiles = await getProfiles(userIds); // Single query
```

**Step 4: Caching Implementation**

Implement proper caching strategies:

**Client-side caching checklist:**
- [ ] SWR or React Query for API caching
- [ ] Proper cache invalidation strategies
- [ ] Optimistic updates for better UX
- [ ] Local storage for persistent data
- [ ] Service worker caching for offline
- [ ] CDN caching for static assets
- [ ] Browser cache headers configured
- [ ] Image optimization and caching

**Server-side caching checklist:**
- [ ] Redis/Memcached for API responses
- [ ] Database query result caching
- [ ] Edge caching with CDN
- [ ] Static generation where possible
- [ ] Incremental Static Regeneration (ISR)
- [ ] Route segment caching (App Router)
- [ ] Proper cache-control headers
- [ ] ETags for conditional requests

**Step 5: Database Query Optimization**

**‚ùå INEFFICIENT - Overfetching:**
```typescript
// Bad: Fetching entire objects
const users = await prisma.user.findMany({
  include: {
    posts: true,
    comments: true,
    profile: true,
    settings: true
  }
});
```

**‚úÖ OPTIMIZED - Select only needed fields:**
```typescript
// Good: Fetch only what's needed
const users = await prisma.user.findMany({
  select: {
    id: true,
    name: true,
    email: true,
    posts: {
      select: {
        title: true,
        createdAt: true
      },
      take: 5 // Limit related data
    }
  }
});
```

**Step 6: Request Batching & Deduplication**

Implement request batching:

```typescript
// Batch multiple requests with DataLoader pattern
const userLoader = new DataLoader(async (userIds) => {
  const users = await prisma.user.findMany({
    where: { id: { in: userIds } }
  });
  return userIds.map(id => users.find(u => u.id === id));
});

// Automatic deduplication and batching
const user1 = await userLoader.load(1);
const user2 = await userLoader.load(2);
const user1Again = await userLoader.load(1); // Uses cached result
```

**Step 7: API Cost Optimization**

Reduce API usage costs:

- [ ] Implement request quotas and limits
- [ ] Use webhooks instead of polling
- [ ] Cache expensive API responses
- [ ] Batch operations when possible
- [ ] Use more efficient endpoints
- [ ] Implement retry with backoff
- [ ] Monitor API usage metrics
- [ ] Set up usage alerts

**Next.js Specific Optimizations:**

- [ ] Use Server Components for data fetching
- [ ] Implement proper data fetching patterns
- [ ] Use Route Handlers efficiently
- [ ] Leverage ISR for static content
- [ ] Optimize Image component usage
- [ ] Use Edge Runtime where appropriate
- [ ] Implement proper error boundaries
- [ ] Configure revalidation strategies

**Common Optimization Patterns:**

**1. Implement Data Loaders:**
```typescript
// Centralized data loading with caching
export const loaders = {
  user: new DataLoader(fetchUsers),
  posts: new DataLoader(fetchPosts),
  comments: new DataLoader(fetchComments)
};
```

**2. Use Proper Pagination:**
```typescript
// ‚úÖ Cursor-based pagination
const { data, hasMore } = await fetchPosts({
  cursor: lastPostId,
  limit: 20
});
```

**3. Implement Field-Level Caching:**
```typescript
// Cache individual fields that change rarely
const user = await cache.wrap(
  `user:${id}:profile`,
  () => fetchUserProfile(id),
  { ttl: 3600 } // 1 hour cache
);
```

**4. Optimize Real-time Updates:**
```typescript
// Use WebSockets for real-time data
const socket = io();
socket.on('update', (data) => {
  // Update only changed data
  queryClient.setQueryData(['posts', data.id], data);
});
```

**Monitoring & Metrics Requirements:**

- [ ] Track API response times
- [ ] Monitor database query performance
- [ ] Set up cost tracking for APIs
- [ ] Alert on performance degradation
- [ ] Log slow queries
- [ ] Track cache hit rates
- [ ] Monitor request volumes
- [ ] Analyze usage patterns

**Failure Response Protocol:**

When inefficiencies are found:

1. **IMMEDIATELY SPAWN AGENTS** to optimize in parallel:
   ```
   "I found 47 duplicate API calls, 23 N+1 queries, and 15 uncached endpoints. I'll spawn agents to optimize:
   - Agent 1: Consolidate duplicate API calls in components/
   - Agent 2: Implement caching for user data endpoints
   - Agent 3: Fix N+1 queries in post loading
   - Agent 4: Batch API requests in dashboard
   - Agent 5: Optimize database indexes
   Let me tackle all of these in parallel..."
   ```
2. **OPTIMIZE EVERYTHING** - Address EVERY inefficiency
3. **VERIFY** - Re-run performance checks after optimizations
4. **REPEAT** - If new issues found, spawn more agents
5. **NO STOPPING** - Keep working until ALL metrics show ‚úÖ OPTIMAL
6. **NO EXCUSES** - Common invalid excuses:
   - "The API doesn't support batching" ‚Üí Implement client-side batching
   - "Caching might show stale data" ‚Üí Implement smart invalidation
   - "The queries are already fast" ‚Üí Make them faster
   - "It's premature optimization" ‚Üí It's necessary optimization
   - "The cost is negligible" ‚Üí Every cent counts

**Performance Benchmarks:**

The code is optimized when:
‚úì Zero duplicate API requests
‚úì All queries < 100ms execution time
‚úì Cache hit rate > 80%
‚úì No N+1 query problems
‚úì API costs reduced by > 50%
‚úì Page load time < 3 seconds
‚úì Time to interactive < 5 seconds
‚úì All lighthouse scores > 90
‚úì Database connection pool optimized
‚úì All endpoints have caching
‚úì Request payloads minimized
‚úì All tests still pass

**Final Optimization Commitment:**

I will now execute EVERY optimization check listed above and FIX ALL INEFFICIENCIES. I will:

- ‚úÖ Analyze all API and database usage patterns
- ‚úÖ Read API documentation to understand best practices
- ‚úÖ SPAWN MULTIPLE AGENTS to optimize in parallel
- ‚úÖ Keep working until EVERYTHING is efficient
- ‚úÖ Verify with performance metrics
- ‚úÖ Not stop until all checks show optimal status

I will NOT:

- ‚ùå Just report inefficiencies without fixing them
- ‚ùå Skip any optimization opportunities
- ‚ùå Accept "good enough" performance
- ‚ùå Ignore potential cost savings
- ‚ùå Leave any redundant calls
- ‚ùå Stop working while inefficiencies remain

**REMEMBER: This is an OPTIMIZATION task, not a reporting task!**

The code is optimized ONLY when every single metric shows ‚úÖ OPTIMAL.

**Executing comprehensive API/database audit and OPTIMIZING ALL CALLS NOW...**